{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# --- QUESTION 4: RAG Pipeline Setup ---\n\nprint(\"---  Setting up RAG Environment ---\")\n\n# 1. Install RAG Libraries\n# rank_bm25: The search engine algorithm\n# sentence-transformers: For vector retrieval (Task 4b)\n# accelerate: Helps load FLAN-T5 efficiently\n!pip install rank_bm25 sentence-transformers transformers datasets torch scikit-learn accelerate > /dev/null\n\n# 2. Download Spacy (for text processing)\n!python -m spacy download en_core_web_sm > /dev/null\n\nimport torch\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"\\n✅ Setup Complete.\")\nprint(f\"Device: {device}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:57:50.717492Z","iopub.execute_input":"2025-11-28T12:57:50.717732Z","iopub.status.idle":"2025-11-28T12:59:14.329812Z","shell.execute_reply.started":"2025-11-28T12:57:50.717709Z","shell.execute_reply":"2025-11-28T12:59:14.328800Z"}},"outputs":[{"name":"stdout","text":"---  Setting up RAG Environment ---\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\npylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\ncudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\nbigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.2.0 which is incompatible.\nlibcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\ncudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\npylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m\n✅ Setup Complete.\nDevice: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# --- STEP 2 & 3 : Load Data, Build KB, and Initialize Retriever ---\nfrom datasets import load_dataset\nfrom rank_bm25 import BM25Okapi\nimport time\n\n#  Load HotpotQA\nprint(\"1. Loading HotpotQA Dataset (this takes a minute)...\")\ndataset = load_dataset(\"hotpot_qa\", \"distractor\", split=\"train[:2000]\") \nprint(f\"    Loaded {len(dataset)} samples.\")\n\n#  Build Knowledge Base (Corpus) & Ground Truths\nprint(\"2. Structuring Knowledge Base & Ground Truths...\")\n\ncorpus = []       \ncorpus_ids = []   \nqueries = []      \nanswers = []      \nground_truth_titles = [] \n\nfor item in dataset:\n    queries.append(item['question'])\n    answers.append(item['answer'])\n    \n    # Extract Ground Truth Titles \n    supp_titles = set(item['supporting_facts']['title'])\n    ground_truth_titles.append(supp_titles)\n    \n    # Extract Paragraphs for Search\n    titles = item['context']['title']\n    sentences = item['context']['sentences']\n    \n    for title, sent_list in zip(titles, sentences):\n        text = \" \".join(sent_list)\n        if text not in corpus:\n            corpus.append(text)\n            corpus_ids.append(title)\n\nprint(f\"   Knowledge Base Built. Documents: {len(corpus)}\")\nprint(f\"   Ground Truths Ready: {len(ground_truth_titles)}\")\n\n#  Build Retriever (BM25)\nprint(\"3. Tokenizing corpus for BM25...\")\ntokenized_corpus = [doc.split(\" \") for doc in corpus]\n\nprint(\"   Indexing corpus...\")\nstart_time = time.time()\nbm25 = BM25Okapi(tokenized_corpus)\nend_time = time.time()\nprint(f\"   ✅ Retriever Ready (took {end_time - start_time:.2f}s).\")\n\n#  Retrieval Helper Function\ndef retrieve_documents(query, k=3):\n    tokenized_query = query.split(\" \")\n    return bm25.get_top_n(tokenized_query, corpus, n=k)\n\n# Test it\nprint(\"\\n--- Test Retrieval ---\")\ntest_q = queries[0]\ndocs = retrieve_documents(test_q)\nprint(f\"Q: {test_q}\")\nprint(f\"Top Result: {docs[0][:150]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:59:14.331708Z","iopub.execute_input":"2025-11-28T12:59:14.332094Z","iopub.status.idle":"2025-11-28T12:59:28.273590Z","shell.execute_reply.started":"2025-11-28T12:59:14.332071Z","shell.execute_reply":"2025-11-28T12:59:28.272831Z"}},"outputs":[{"name":"stdout","text":"1. Loading HotpotQA Dataset (this takes a minute)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8abfc87231b849008e442efeb98f887e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"distractor/train-00000-of-00002.parquet:   0%|          | 0.00/166M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3bf7f4b2c2a34dfebeb6644111492848"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"distractor/train-00001-of-00002.parquet:   0%|          | 0.00/166M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"932384419591413aa9dd5b820cfa17da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"distractor/validation-00000-of-00001.par(…):   0%|          | 0.00/27.5M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"72f53d57849f446fab997674fca8450a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/90447 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ae0fa7932f34b3ca2bfb64a9e350e6e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/7405 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5f096b4783b4ade84019eb485f2bf36"}},"metadata":{}},{"name":"stdout","text":"    Loaded 2000 samples.\n2. Structuring Knowledge Base & Ground Truths...\n   Knowledge Base Built. Documents: 19214\n   Ground Truths Ready: 2000\n3. Tokenizing corpus for BM25...\n   Indexing corpus...\n   ✅ Retriever Ready (took 0.69s).\n\n--- Test Retrieval ---\nQ: Which magazine was started first Arthur's Magazine or First for Women?\nTop Result: First for Women is a woman's magazine published by Bauer Media Group in the USA.  The magazine was started in 1989.  It is based in Englewood Cliffs, ...\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# --- STEP 4 : The Generator (FLAN-T5) ---\nimport os\n# Fix for the \"AttributeError\" noise\nos.environ['PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION'] = 'python'\n\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport torch\nimport random\n\n# Define Device\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Using device: {device}\")\n\n# Load FLAN-T5\n# If the download looks \"stuck\" at 100%, IT IS WORKING. Just wait 1-2 mins for it to verify the file.\nmodel_name = \"google/flan-t5-base\"\nprint(f\"Loading {model_name}...\")\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_name, device_map=\"auto\")\nprint(\"✅ Generator Loaded.\")\n\n# Define Generation Function\ndef generate_answer(question, context_docs):\n    context_text = \" \".join(context_docs)\n    input_text = f\"question: {question} context: {context_text}\"\n    \n    input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=1024).input_ids.to(device)\n    \n    outputs = model.generate(input_ids, max_length=128, num_beams=4, early_stopping=True)\n    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return answer\n\n# The Full RAG Pipeline\ndef run_rag(question):\n    retrieved_docs = retrieve_documents(question, k=3)\n    answer = generate_answer(question, retrieved_docs)\n    return answer, retrieved_docs\n\n# --- TEST THE SYSTEM (5 Random Examples) ---\nprint(\"\\n---  RAG System Test (5 Random Samples) ---\")\n\ntest_indices = random.sample(range(len(queries)), 5)\n\nfor i, idx in enumerate(test_indices):\n    question = queries[idx]\n    true_answer = answers[idx]\n    \n    print(f\"\\n[Example {i+1}]\")\n    print(f\"Q: {question}\")\n    \n    pred_ans, docs = run_rag(question)\n    \n    print(f\"True Answer: {true_answer}\")\n    print(f\"RAG Answer:  {pred_ans}\")\n    print(\"-\" * 40)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T12:59:28.274302Z","iopub.execute_input":"2025-11-28T12:59:28.274737Z","iopub.status.idle":"2025-11-28T13:00:00.518136Z","shell.execute_reply.started":"2025-11-28T12:59:28.274716Z","shell.execute_reply":"2025-11-28T13:00:00.517488Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nLoading google/flan-t5-base...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b347aeab2d34c4387108cedc74db882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a5a68ce4c89341969d4636786f0639d3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28bbad1ebf8b4c0c904088268ef5d0a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b4c4a0adf32746df802c4148c1dc173d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bd5c211c7b0428595d3aa7f01128f00"}},"metadata":{}},{"name":"stderr","text":"2025-11-28 12:59:38.126663: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764334778.280052     117 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764334778.322830     117 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"925008873148481ba6d8974289275bb5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9e072a3cfce84b2499bb6c5930e7b7a9"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/accelerate/utils/modeling.py:1614: UserWarning: The following device_map keys do not match any submodules in the model: ['decoder.embed_tokens', 'encoder.embed_tokens']\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"✅ Generator Loaded.\n\n---  RAG System Test (5 Random Samples) ---\n\n[Example 1]\nQ: Kerrygold Irish Cream Liqueur is owned by a company that mainly sells what type or product?\nTrue Answer: dairy products\nRAG Answer:  liqueur\n----------------------------------------\n\n[Example 2]\nQ: Were Lee Ji-hye and Shim Eun-jin in the same band?\nTrue Answer: no\nRAG Answer:  no\n----------------------------------------\n\n[Example 3]\nQ: Where is the Peace Palace Library located?\nTrue Answer: It is located in The Hague, Netherlands, and was established to support the Permanent Court of Justice\nRAG Answer:  The Hague\n----------------------------------------\n\n[Example 4]\nQ: Scrabble and Sentinels of the Multiverse, is which form of entertainment?\nTrue Answer: game\nRAG Answer:  board game\n----------------------------------------\n\n[Example 5]\nQ: A 2001 Indian epic sports-drama film starred an actor who was also know for his role in what 1999 movie?\nTrue Answer: Sarfarosh\nRAG Answer:  The Secret Life of Girls\n----------------------------------------\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- STEP 5 : Full RAG Evaluation ---\n\n#  Install Metric Libraries\nprint(\"Installing evaluation libraries...\")\n!pip install torchmetrics bert_score > /dev/null\n\nfrom torchmetrics.text.rouge import ROUGEScore\nfrom torchmetrics.text.bleu import BLEUScore\nfrom torchmetrics.text.bert import BERTScore\nimport torch\nimport numpy as np\n\n#  Initialize Metrics\nprint(\"Loading Metrics (this may take a moment)...\")\nrouge = ROUGEScore()\nbleu = BLEUScore()\n# Use distilbert for speed\nbert_scorer = BERTScore(model_name_or_path='distilbert-base-uncased', device=device)\nprint(\" Metrics Ready.\")\n\n#  Helper: Retrieve Indices\ndef retrieve_indices(query, k=3):\n    tokenized_query = query.split(\" \")\n    scores = bm25.get_scores(tokenized_query)\n    top_n_indices = np.argsort(scores)[::-1][:k]\n    return top_n_indices\n\n#  Evaluation Loop\ndef evaluate_rag(num_samples=50):\n    print(f\"\\n---  Evaluating RAG Pipeline ({num_samples} samples) ---\")\n    \n    retrieval_hits = 0\n    total_gold_docs = 0\n    \n    preds = []\n    targets = []\n    \n    for i in range(num_samples):\n        if i % 10 == 0: print(\".\", end=\"\", flush=True)\n        \n        query = queries[i]\n        gold_answer = answers[i]\n        gold_titles = ground_truth_titles[i]\n        \n        # A. Retrieve\n        top_indices = retrieve_indices(query, k=3)\n        retrieved_titles = [corpus_ids[idx] for idx in top_indices]\n        retrieved_texts = [corpus[idx] for idx in top_indices]\n        \n        # Calculate Recall\n        hits = sum([1 for t in retrieved_titles if t in gold_titles])\n        retrieval_hits += hits\n        total_gold_docs += len(gold_titles)\n        \n        # B. Generate\n        pred_ans = generate_answer(query, retrieved_texts)\n        \n        preds.append(pred_ans)\n        targets.append(gold_answer)\n\n    print(\" Done!\")\n    \n    #  Calculate Scores\n    \n    # Retrieval Score\n    recall = retrieval_hits / total_gold_docs if total_gold_docs > 0 else 0\n    print(f\"\\n Retrieval Recall@3: {recall:.2%}\")\n    \n    # Generation Scores\n    print(\" Calculating Generation Metrics...\")\n    \n    # BLEU\n    bleu_targets = [[t] for t in targets]\n    b_score = bleu(preds, bleu_targets).item() * 100\n    \n    # ROUGE\n    r_score = rouge(preds, targets)\n    \n    # BERTScore\n    bert_vals = bert_scorer(preds, targets)\n    bert_f1 = bert_vals['f1'].mean().item() * 100\n    \n    print(f\" BLEU:       {b_score:.2f}\")\n    print(f\" ROUGE-L:    {r_score['rougeL_fmeasure'].item()*100:.2f}\")\n    print(f\" BERTScore:  {bert_f1:.2f}\")\n    \n    return preds, targets\n\n# Run Evaluation on 50 samples\ngenerated_answers, true_answers = evaluate_rag(num_samples=50)\n\n# --- Qualitative Analysis (Task 4d) ---\nprint(\"\\n---  Qualitative Analysis (5 Examples) ---\")\nfor i in range(5): \n    print(f\"\\n[Example {i+1}]\")\n    print(f\"Q: {queries[i]}\")\n    print(f\"True: {true_answers[i]}\")\n    print(f\"RAG:  {generated_answers[i]}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T13:00:00.518866Z","iopub.execute_input":"2025-11-28T13:00:00.519479Z","iopub.status.idle":"2025-11-28T13:00:28.411649Z","shell.execute_reply.started":"2025-11-28T13:00:00.519434Z","shell.execute_reply":"2025-11-28T13:00:28.410920Z"}},"outputs":[{"name":"stdout","text":"Installing evaluation libraries...\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Loading Metrics (this may take a moment)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9db3246d1a8a46fc82ca434f900a93a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d239294f2f234a45b57058954289b04c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5caca8e6cc754fd49dfb5bb5afb9347a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5da43f06dffb46cf9fc8de72ffbc7214"}},"metadata":{}},{"name":"stdout","text":" Metrics Ready.\n\n---  Evaluating RAG Pipeline (50 samples) ---\n..... Done!\n\n Retrieval Recall@3: 44.00%\n Calculating Generation Metrics...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"342aae435f044b5fa4dbcec5eb6c04cc"}},"metadata":{}},{"name":"stderr","text":"The following layers were not sharded: embeddings.position_embeddings.weight, transformer.layer.*.sa_layer_norm.weight, transformer.layer.*.ffn.lin*.weight, transformer.layer.*.attention.out_lin.weight, transformer.layer.*.sa_layer_norm.bias, transformer.layer.*.attention.q_lin.bias, transformer.layer.*.attention.v_lin.bias, transformer.layer.*.ffn.lin*.bias, embeddings.LayerNorm.bias, transformer.layer.*.output_layer_norm.bias, transformer.layer.*.attention.k_lin.bias, embeddings.word_embeddings.weight, transformer.layer.*.attention.out_lin.bias, transformer.layer.*.output_layer_norm.weight, transformer.layer.*.attention.k_lin.weight, transformer.layer.*.attention.v_lin.weight, transformer.layer.*.attention.q_lin.weight, embeddings.LayerNorm.weight\n","output_type":"stream"},{"name":"stdout","text":" BLEU:       0.00\n ROUGE-L:    40.60\n BERTScore:  46.24\n\n---  Qualitative Analysis (5 Examples) ---\n\n[Example 1]\nQ: Which magazine was started first Arthur's Magazine or First for Women?\nTrue: Arthur's Magazine\nRAG:  First for Women\n\n[Example 2]\nQ: The Oberoi family is part of a hotel company that has a head office in what city?\nTrue: Delhi\nRAG:  Delhi\n\n[Example 3]\nQ: Musician and satirist Allie Goertz wrote a song about the \"The Simpsons\" character Milhouse, who Matt Groening named after who?\nTrue: President Richard Nixon\nRAG:  President Richard Nixon\n\n[Example 4]\nQ:  What nationality was James Henry Miller's wife?\nTrue: American\nRAG:  Jamaican\n\n[Example 5]\nQ: Cadmium Chloride is slightly soluble in this chemical, it is also called what?\nTrue: alcohol\nRAG:  Benzamide\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# --- STEP 6: Build Dense Retriever (Sentence-BERT) ---\nfrom sentence_transformers import SentenceTransformer, util\nimport torch\nimport time\n\n# Load Model\nprint(\"Loading Sentence-BERT model...\")\ndense_model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n\n# Encode Corpus (The Knowledge Base)\nprint(\"Encoding Corpus ...\")\nstart_time = time.time()\n\n# We encode the corpus list we built in Step 2\ncorpus_embeddings = dense_model.encode(corpus, convert_to_tensor=True, show_progress_bar=True)\n\nend_time = time.time()\nprint(f\" Corpus Encoded in {end_time - start_time:.2f} seconds.\")\nprint(f\" Embedding Shape: {corpus_embeddings.shape}\")\n\n# Define Retrieval Function\ndef retrieve_dense(query, k=3):\n    # Encode query\n    query_embedding = dense_model.encode(query, convert_to_tensor=True)\n    \n    # Semantic Search (Cos Sim)\n    hits = util.semantic_search(query_embedding, corpus_embeddings, top_k=k)\n    \n    # hits is a list of lists (one per query). We only have 1 query.\n    top_hits = hits[0]\n    \n    # Return indices\n    return [hit['corpus_id'] for hit in top_hits]\n\n# Test\nprint(\"\\n--- Dense Retrieval Test ---\")\ntest_q = \"Who directed the movie 'Silence of the Lambs'?\" # Out of domain test\ndocs_idx = retrieve_dense(test_q, k=1)\nprint(f\"Q: {test_q}\")\nprint(f\"Top Result: {corpus[docs_idx[0]][:200]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T13:00:28.412514Z","iopub.execute_input":"2025-11-28T13:00:28.412740Z","iopub.status.idle":"2025-11-28T13:00:55.890494Z","shell.execute_reply.started":"2025-11-28T13:00:28.412722Z","shell.execute_reply":"2025-11-28T13:00:55.889600Z"}},"outputs":[{"name":"stdout","text":"Loading Sentence-BERT model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3e2b3cd8b6c4188a762f154d3238ef4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b5d8858f615f4ad289e5845dc27146ef"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c84d60cd1d99403eae1f623e86539307"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf686aef19ff45d0a4e9b7cf19712385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34f532fbc7ca4a869fc59a36d1b18bd1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0e0621404ff48da8c9142e4963c0cc5"}},"metadata":{}},{"name":"stderr","text":"The following layers were not sharded: encoder.layer.*.attention.self.query.bias, embeddings.position_embeddings.weight, encoder.layer.*.output.dense.weight, encoder.layer.*.attention.output.LayerNorm.weight, pooler.dense.weight, encoder.layer.*.attention.output.LayerNorm.bias, encoder.layer.*.output.dense.bias, encoder.layer.*.attention.output.dense.bias, encoder.layer.*.attention.self.query.weight, encoder.layer.*.output.LayerNorm.bias, pooler.dense.bias, encoder.layer.*.attention.output.dense.weight, embeddings.LayerNorm.bias, embeddings.token_type_embeddings.weight, embeddings.word_embeddings.weight, encoder.layer.*.attention.self.value.bias, encoder.layer.*.attention.self.key.weight, encoder.layer.*.attention.self.value.weight, encoder.layer.*.attention.self.key.bias, encoder.layer.*.intermediate.dense.bias, embeddings.LayerNorm.weight, encoder.layer.*.output.LayerNorm.weight, encoder.layer.*.intermediate.dense.weight\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"481c0c6b28314c24b19ece2d073657eb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7b8ff7dbbb5040b4a52ab83b57ff6a92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69159440c74349f0a3593041b168a841"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e88e267ffcf842048752d5c157097979"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfb5999fdc0f4cdba8b8be78c23972d5"}},"metadata":{}},{"name":"stdout","text":"Encoding Corpus ...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/601 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03d36f6ea71e4275bd626cf651707cf5"}},"metadata":{}},{"name":"stdout","text":" Corpus Encoded in 23.15 seconds.\n Embedding Shape: torch.Size([19214, 384])\n\n--- Dense Retrieval Test ---\nQ: Who directed the movie 'Silence of the Lambs'?\nTop Result: The Silence of the Lambs is a 1991 American horror-thriller film directed by Jonathan Demme and starring Jodie Foster, Anthony Hopkins, and Scott Glenn.  Adapted by Ted Tally from the 1988 novel of th...\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# --- STEP 7 : Final Comparison (BM25 vs Dense) ---\nimport numpy as np\nimport pandas as pd\n\n# Helper: Calculate Precision/Recall for Retrieval\ndef calculate_retrieval_metrics(retrieved_titles, gold_titles):\n    if len(retrieved_titles) == 0: return 0.0, 0.0\n    \n    # Hits (Is the gold title in our retrieved list?)\n    hits = sum([1 for t in retrieved_titles if t in gold_titles])\n    \n    # Precision = Hits / Retrieved\n    precision = hits / len(retrieved_titles)\n    \n    # Recall = Hits / Total Relevant\n    recall = hits / len(gold_titles) if len(gold_titles) > 0 else 0.0\n    \n    return precision, recall\n\ndef evaluate_pipeline(name, retrieve_fn, num_samples=50):\n    print(f\"\\n--- Evaluating {name} ({num_samples} samples) ---\")\n    \n    # Metric Accumulators\n    total_precision = 0\n    total_recall = 0\n    preds = []\n    targets = []\n    \n    print(\"Running...\", end=\"\", flush=True)\n    for i in range(num_samples):\n        if i % 10 == 0: print(\".\", end=\"\", flush=True)\n        \n        query = queries[i]\n        gold_ans = answers[i]\n        gold_set = ground_truth_titles[i]\n        \n        # RETRIEVE\n        if name == \"BM25\":\n            top_indices = retrieve_indices(query, k=3)\n        else:\n            top_indices = retrieve_fn(query, k=3)\n            \n        # Get titles and text\n        ret_titles = [corpus_ids[idx] for idx in top_indices]\n        ret_texts = [corpus[idx] for idx in top_indices]\n        \n        # Calculate P/R\n        p, r = calculate_retrieval_metrics(ret_titles, gold_set)\n        total_precision += p\n        total_recall += r\n        \n        # GENERATE\n        pred_ans = generate_answer(query, ret_texts)\n        preds.append(pred_ans)\n        targets.append(gold_ans)\n        \n    print(\" Done!\")\n    \n    # Averages\n    avg_p = total_precision / num_samples\n    avg_r = total_recall / num_samples\n    \n    # Generation Metrics\n    bleu_targets = [[t] for t in targets]\n    \n    b_score = bleu(preds, bleu_targets).item() * 100\n    r_score = rouge(preds, targets)['rougeL_fmeasure'].item() * 100\n    bert_f1 = bert_scorer(preds, targets)['f1'].mean().item() * 100\n    \n    return {\n        \"Precision@3\": avg_p * 100,\n        \"Recall@3\": avg_r * 100,\n        \"BLEU\": b_score,\n        \"ROUGE-L\": r_score,\n        \"BERTScore\": bert_f1\n    }\n\n# --- RUN COMPARISON ---\nresults_table = {}\n\n# 1. Evaluate BM25\nresults_table[\"BM25 + FLAN-T5\"] = evaluate_pipeline(\"BM25\", None, num_samples=50)\n\n# 2. Evaluate Dense\nresults_table[\"Dense + FLAN-T5\"] = evaluate_pipeline(\"Dense\", retrieve_dense, num_samples=50)\n\n# Print Final Table\nprint(\"\\n\\n===  FINAL RAG SCOREBOARD ===\")\ndf = pd.DataFrame(results_table).T\nprint(df.round(2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-28T13:00:55.891395Z","iopub.execute_input":"2025-11-28T13:00:55.892071Z","iopub.status.idle":"2025-11-28T13:01:21.339734Z","shell.execute_reply.started":"2025-11-28T13:00:55.892041Z","shell.execute_reply":"2025-11-28T13:01:21.338960Z"}},"outputs":[{"name":"stdout","text":"\n--- Evaluating BM25 (50 samples) ---\nRunning........ Done!\n","output_type":"stream"},{"name":"stderr","text":"The following layers were not sharded: embeddings.position_embeddings.weight, transformer.layer.*.sa_layer_norm.weight, transformer.layer.*.ffn.lin*.weight, transformer.layer.*.attention.out_lin.weight, transformer.layer.*.sa_layer_norm.bias, transformer.layer.*.attention.q_lin.bias, transformer.layer.*.attention.v_lin.bias, transformer.layer.*.ffn.lin*.bias, embeddings.LayerNorm.bias, transformer.layer.*.output_layer_norm.bias, transformer.layer.*.attention.k_lin.bias, embeddings.word_embeddings.weight, transformer.layer.*.attention.out_lin.bias, transformer.layer.*.output_layer_norm.weight, transformer.layer.*.attention.k_lin.weight, transformer.layer.*.attention.v_lin.weight, transformer.layer.*.attention.q_lin.weight, embeddings.LayerNorm.weight\n","output_type":"stream"},{"name":"stdout","text":"\n--- Evaluating Dense (50 samples) ---\nRunning........ Done!\n","output_type":"stream"},{"name":"stderr","text":"The following layers were not sharded: embeddings.position_embeddings.weight, transformer.layer.*.sa_layer_norm.weight, transformer.layer.*.ffn.lin*.weight, transformer.layer.*.attention.out_lin.weight, transformer.layer.*.sa_layer_norm.bias, transformer.layer.*.attention.q_lin.bias, transformer.layer.*.attention.v_lin.bias, transformer.layer.*.ffn.lin*.bias, embeddings.LayerNorm.bias, transformer.layer.*.output_layer_norm.bias, transformer.layer.*.attention.k_lin.bias, embeddings.word_embeddings.weight, transformer.layer.*.attention.out_lin.bias, transformer.layer.*.output_layer_norm.weight, transformer.layer.*.attention.k_lin.weight, transformer.layer.*.attention.v_lin.weight, transformer.layer.*.attention.q_lin.weight, embeddings.LayerNorm.weight\n","output_type":"stream"},{"name":"stdout","text":"\n\n===  FINAL RAG SCOREBOARD ===\n                 Precision@3  Recall@3  BLEU  ROUGE-L  BERTScore\nBM25 + FLAN-T5         29.33      44.0   0.0    40.60      46.24\nDense + FLAN-T5        44.00      66.0   0.0    49.67      48.76\n","output_type":"stream"}],"execution_count":6}]}